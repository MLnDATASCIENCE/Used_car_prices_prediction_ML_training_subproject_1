{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504d408a",
   "metadata": {
    "id": "504d408a"
   },
   "source": [
    "# Subproject 1 – Used Car Prices Prediction\n",
    "Machine Learning – M.Sc. in Electrical and Computer Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9be13e",
   "metadata": {
    "id": "0d9be13e"
   },
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b568a7",
   "metadata": {
    "id": "a0b568a7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf95a5",
   "metadata": {
    "id": "06cf95a5"
   },
   "source": [
    "Reading files using pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168fa99",
   "metadata": {
    "collapsed": true,
    "id": "e168fa99"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_sample_submission = pd.read_csv(\"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQWwe-0pRESp",
   "metadata": {
    "id": "bQWwe-0pRESp"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GF_SWkO0gLI8",
   "metadata": {
    "id": "GF_SWkO0gLI8"
   },
   "source": [
    "using the shape() method, will give the dimension of the data i am working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GNb6qxxeTLYv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GNb6qxxeTLYv",
    "outputId": "ffd76408-f25e-49bc-b0b9-320a3cc9c81c"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oBX72suhgRDj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oBX72suhgRDj",
    "outputId": "d58052af-ba98-48cb-bea1-803bf6768c01"
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gsh6tq13eFHX",
   "metadata": {
    "id": "Gsh6tq13eFHX"
   },
   "source": [
    "the head() method will show first rows of data but limited, \"n\" can be passed as an argument, if we want specific amount to row to be shown.\n",
    "passing n as an argument here will show the first n rows from index (0 to n-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JpdNp4RITee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "JpdNp4RITee3",
    "outputId": "48a83bc2-fd2f-4bbb-afbe-20d7c374a7e5"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeZWj74reXA7",
   "metadata": {
    "id": "aeZWj74reXA7"
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdbea8",
   "metadata": {
    "id": "46bdbea8"
   },
   "source": [
    "Using info() method to see the summary details about the dataframes like the index, datatypes, columns, non-null values and the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e00ebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "36e00ebf",
    "outputId": "cb5e0ef3-0ca2-450d-f586-f02e0f6c1958"
   },
   "outputs": [],
   "source": [
    "#  for the train data\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P5WljiTidc41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "P5WljiTidc41",
    "outputId": "2afba4ef-c170-490e-82ea-e065319e3b28"
   },
   "outputs": [],
   "source": [
    "#  for the test data\n",
    "\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcxzaS76nypp",
   "metadata": {
    "id": "qcxzaS76nypp"
   },
   "source": [
    "checking the data types of the data we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0_K3gCNn9WP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "collapsed": true,
    "id": "e0_K3gCNn9WP",
    "outputId": "d6b6cf15-bb60-4a4f-b9ea-73abf15b4931"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s33AjwBdoM7C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "collapsed": true,
    "id": "s33AjwBdoM7C",
    "outputId": "ecb9e002-4f43-43bb-d63b-d39606a944f5"
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b1f80",
   "metadata": {},
   "source": [
    "the describitution of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the distribution plot below, we can say that the the target ['price'] of the train data is skewed with fewer high-priced car.\n",
    "\n",
    "# The target distribution was analyzed to identify skewness and outliers in used car prices, which directly affects model choice and potential target transformations.\n",
    "\n",
    "# price distrubution  visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_train[\"price\"], bins=50)\n",
    "plt.title(\"Distribution of Car Prices\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U6gWPXDZXCEK",
   "metadata": {
    "id": "U6gWPXDZXCEK"
   },
   "source": [
    "separating numerical column and categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxNAKIhCW_82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxNAKIhCW_82",
    "outputId": "24bb805b-0b9c-4e3d-c269-051ea22aa2ea"
   },
   "outputs": [],
   "source": [
    "#  for train data\n",
    "\n",
    "numerical_columns = df_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_columns = df_test.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns\n",
    "\n",
    "print(\"Numerical columns:\", numerical_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this plot will show the 'brand' category showing that a small number of dataset dominate compare to other brands\n",
    "\n",
    "# this was conducted to identify dominant and rare brands, guiding the choice of encoding strategies and dimensionality control. wecan also change the column to other categorical columns too for better encoding strategies.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_train[\"brand\"].value_counts().head(10).plot(kind=\"bar\")\n",
    "plt.title(\"Top 10 Car Brands\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between price and fuel type was analyzed to verify its influence on vehicle value and justify categorical encoding.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_train.boxplot(column=\"price\", by=\"fuel_type\")\n",
    "plt.title(\"Price vs Fuel Type\")\n",
    "plt.suptitle(\"\")  # removes automatic subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXcH7MzsiR5z",
   "metadata": {
    "id": "cXcH7MzsiR5z"
   },
   "source": [
    "Missing values\n",
    "\n",
    "the pandas library provides isnull() to check the column with missing values, sum() will give the total sum of the missing values if any exist,  and the sort_values for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1QV6bKFXidYL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "1QV6bKFXidYL",
    "outputId": "081f9fee-4f40-44d1-bd5e-c8a05d78b599"
   },
   "outputs": [],
   "source": [
    "#  for train data\n",
    "\n",
    "train_missing_values =df_train.isnull().sum().sort_values(ascending=False)\n",
    "train_missing_columns = df_train.columns[df_train.isna().any()].tolist()\n",
    "train_missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htMglwcbiqbp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "htMglwcbiqbp",
    "outputId": "a5836513-ced2-46c8-e146-a678e94c9564"
   },
   "outputs": [],
   "source": [
    "#for test data\n",
    "\n",
    "df_test.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas dataframe, we can use the .mean() method to find the mean for the each colums with missing values, then convert it to percentage multiplying be 100.\n",
    "\n",
    "#  Missing value analysis was performed to quantify data incompleteness and justify the imputation strategies applied during preprocessing.\n",
    "\n",
    "train_missing_percentage = df_train.isnull().mean().sort_values(ascending=False) * 100\n",
    "train_missing_percentage[train_missing_percentage > 0]\n",
    "\n",
    "# the plot for columns with missing data\n",
    "(df_train.drop(['price'], axis=1).isnull().mean() * 100).plot(kind=\"bar\")\n",
    "plt.title(\"Missing values\")\n",
    "plt.ylabel(\"missing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f837fd9",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc944a",
   "metadata": {},
   "source": [
    "## Filling missing data\n",
    "\n",
    "From using info() method when doing EDA above on the train data, we can see that there are some missing values. Like in the fuel_type, accident and the clean_title features.\n",
    "This missing values are categorical features, therefore there is need to fill the the missing values before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the missing values in the training set with its modes using the mode() function to get the mode and using fillna() function to fill the positions with misiing data. The 2 method are from pandas library \n",
    "\n",
    "for column in train_missing_columns:\n",
    "    mode_value = df_train[column].mode()[0]\n",
    "    df_train[column] = df_train[column].fillna(mode_value)\n",
    "\n",
    "# For the test set, i am only fill missing values for columns that are present in the train data set and excluding 'price'.\n",
    "# DOing this bring cosistency and more accuracy when training the train data. As we are dealing with unseeb data, we can't be sure if the unseen data will have additional column or not, or if they will contain some missing data, so it is best to take care of that. \n",
    " \n",
    " \n",
    "missing_columns_test = [col for col in train_missing_columns if col in df_test.columns]\n",
    "\n",
    "for column in missing_columns_test:\n",
    "    # Use the mode calculated from the training set for consistency\n",
    "    mode_value = df_train[column].mode()[0]\n",
    "    df_test[column] = df_test[column].fillna(mode_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87116650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27103b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230c025",
   "metadata": {},
   "source": [
    "## Encoding data\n",
    "\n",
    " We now have to convert all catogorical features into numerical beacuse most Machine learning models can not work with categorical features.\n",
    "\n",
    "To do this, i will be making use of the get_dummies method that implements OneHotEncoding from the pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f776c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_train.drop(['price'], axis=1)\n",
    "# y = df_train['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb505fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_encoded = pd.get_dummies(df_train, drop_first=True)\n",
    "\n",
    "df_test_encoded = pd.get_dummies(df_test, drop_first=True)\n",
    "df_test_encoded = df_test_encoded.reindex(columns=df_train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2bcbe",
   "metadata": {},
   "source": [
    "spliting the trian data into features (X) and target (y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa302cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_encoded.drop(['price'], axis=1)\n",
    "y = df_train_encoded['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eec337",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784ff00",
   "metadata": {},
   "source": [
    "Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12039e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "base_models = {\n",
    "    \"Linear regression\": LinearRegression(),\n",
    "    \"Decision tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=50,        \n",
    "        max_depth=12,          \n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features=0.4,      \n",
    "        max_samples=0.7,        \n",
    "        n_jobs=-1,              \n",
    "        random_state=42),\n",
    "        \"k-NN\": KNeighborsRegressor(n_neighbors=3)\n",
    "}\n",
    "\n",
    "base_models_information = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval_plot(name, model, ax):\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "\n",
    "    base_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, base_pred))\n",
    "\n",
    "    ax.scatter(y_test, base_pred, c='g', alpha=0.5, label='Predicted')\n",
    "    min_val = min(y_test.min(), base_pred.min())\n",
    "    max_val = max(y_test.max(), base_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], c='r', label='Ideal')\n",
    "\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_title(f'{name} / RMSE={rmse} / score={score}')\n",
    "    ax.legend()\n",
    "\n",
    "    return {\"model\": name, \"rmse\": rmse, \"pred\": base_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e006fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Linear regression\", base_models[\"Linear regression\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea48828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Decision tree\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Decision tree\", base_models[\"Decision tree\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adaf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Random forest\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Random Forest\", base_models[\"Random Forest\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  K-Nearest Neighbors\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"k-NN\", base_models[\"k-NN\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da0a0e",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee371ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_Scaler = StandardScaler().fit(X)\n",
    "\n",
    "df_train_scaled = pd.DataFrame(standard_Scaler.transform(X), columns = X.columns)\n",
    "df_test_scaled = standard_Scaler.fit_transform(df_test_encoded)\n",
    "df_train_scaled.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_train_scaled['price'] =y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151aea8",
   "metadata": {},
   "source": [
    "Models Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677963d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "results = []\n",
    "best_models = {}\n",
    "scoring = \"neg_root_mean_squared_error\"\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "X_scaled = df_train_scaled.drop(['price'], axis=1)\n",
    "y_scaled = df_train_scaled['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size=0.2, random_state=42)\n",
    "\n",
    "def run_gridsearch(name, model, grid, scoring):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        # n_jobs=-1,\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    rmse = -gs.best_score_\n",
    "    results.append({\"model\": name, \"rmse\": rmse, \"params\": gs.best_params_})\n",
    "    best_models[name] = gs.best_estimator_\n",
    "    params = gs.best_params_\n",
    "    print(f\"{name} | RMSE={rmse:.4f}\")\n",
    "    print(f\"params={params} \")\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_grid = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"positive\": [False, True]\n",
    "}\n",
    "\n",
    "run_gridsearch(\"linreg\", LinearRegression(), linreg_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = {\n",
    "    \"n_neighbors\": [5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"minkowski\"],\n",
    "    \"p\": [2],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"knn\", KNeighborsRegressor(), knn_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid = {\n",
    "    \"max_depth\": [None, 10],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"ccp_alpha\": [0.0],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"dt\", DecisionTreeRegressor(random_state=42), dt_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [100],\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"bootstrap\": [True],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"rf\", RandomForestRegressor(random_state=42), rf_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afca2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(\"rmse\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c45e87",
   "metadata": {},
   "source": [
    "Kaggle Submission File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406eb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the best model\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "\n",
    "# Retrieve the best estimator (pipeline) for that model\n",
    "best = best_models[best_model_name]\n",
    "\n",
    "# Preprocess the test set using the same preprocessor fitted on the training data\n",
    "# Note: The preprocess object is already part of the best_pipeline\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best.predict(df_test)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'id': df_test['id'], 'price': predictions})\n",
    "\n",
    "# Ensure prices are non-negative, as car prices cannot be negative\n",
    "submission_df['price'] = submission_df['price'].apply(lambda x: max(0, x))\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV for Kaggle submission\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
