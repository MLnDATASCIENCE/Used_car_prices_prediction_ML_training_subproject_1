{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504d408a",
   "metadata": {
    "id": "504d408a"
   },
   "source": [
    "# Subproject 1 – Used Car Prices Prediction\n",
    "Machine Learning – M.Sc. in Electrical and Computer Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9be13e",
   "metadata": {
    "id": "0d9be13e"
   },
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b568a7",
   "metadata": {
    "id": "a0b568a7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e6db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf95a5",
   "metadata": {
    "id": "06cf95a5"
   },
   "source": [
    "Reading datasets files and making dataframes using pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e168fa99",
   "metadata": {
    "collapsed": true,
    "id": "e168fa99"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_sample_submission = pd.read_csv(\"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQWwe-0pRESp",
   "metadata": {
    "id": "bQWwe-0pRESp"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "After reading and making DataFrames of the Datasets, i will be analysing the data.\n",
    "\n",
    "Purpose: \n",
    "\n",
    "This will let me understand the kind of datatype i am working. \n",
    "It will let me understand the kind of features that are present in the Data, if they are categorical or numerical features.\n",
    "If there there are missing values in the data, and the type (if any). This make me have better decision on methods i can use to fill up the values.\n",
    "\n",
    "To do this i will be making use of some methods from the pandas library such as \"shape()\", \"head()\", \"info()\", \"dtypes()\", \"describe()\". \n",
    "\n",
    "During this EDA, i wil also be exploring some other related and important part which wich will let me make better decision when training tdata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GF_SWkO0gLI8",
   "metadata": {
    "id": "GF_SWkO0gLI8"
   },
   "source": [
    "using the shape() method, will give the dimension (row x columns) of the data i am working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GNb6qxxeTLYv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GNb6qxxeTLYv",
    "outputId": "ffd76408-f25e-49bc-b0b9-320a3cc9c81c"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oBX72suhgRDj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "oBX72suhgRDj",
    "outputId": "d58052af-ba98-48cb-bea1-803bf6768c01"
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gsh6tq13eFHX",
   "metadata": {
    "id": "Gsh6tq13eFHX"
   },
   "source": [
    "the head() method will show first rows of data but limited, \"n\" can be passed as an argument, if we want specific amount of row to be shown.\n",
    "\n",
    "passing n as an argument here will show the first n rows from index (0 to n-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JpdNp4RITee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "JpdNp4RITee3",
    "outputId": "48a83bc2-fd2f-4bbb-afbe-20d7c374a7e5"
   },
   "outputs": [],
   "source": [
    "# for train data\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeZWj74reXA7",
   "metadata": {
    "id": "aeZWj74reXA7"
   },
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdbea8",
   "metadata": {
    "id": "46bdbea8"
   },
   "source": [
    "Using info() method to see the summary details about the dataframes like the index, datatypes, columns, non-null values and the memory usage.\n",
    "\n",
    "Ths method also shows the total numbers of values for each of the columns present in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e00ebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "36e00ebf",
    "outputId": "cb5e0ef3-0ca2-450d-f586-f02e0f6c1958"
   },
   "outputs": [],
   "source": [
    "#  for the train data\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P5WljiTidc41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "P5WljiTidc41",
    "outputId": "2afba4ef-c170-490e-82ea-e065319e3b28"
   },
   "outputs": [],
   "source": [
    "#  for the test data\n",
    "\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b1f80",
   "metadata": {},
   "source": [
    "using describe() will give the descriptive statistics of the dataset such as the mean, count, std and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c734d76",
   "metadata": {},
   "source": [
    "Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  only numerical data can be plot.\n",
    "\n",
    "df_train.drop(['price'], axis=1).plot(kind='box', figsize=(20,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c28e9c",
   "metadata": {},
   "source": [
    "Target visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the price distribution plot below, it is noticed that the target ['price'] of the train data is skewed with fewer high-priced car.\n",
    "\n",
    "# The target distribution was analyzed to identify skewness and outliers in used car prices, which directly affects model choice and potential target transformations.\n",
    "\n",
    "# Due to the skewness from the target, we can make use of log transform, beacuse it compreses the high-price tail and also affect the proportionality of the errors.\n",
    "\n",
    "# price distrubution  visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df_train[\"price\"], bins=50)\n",
    "plt.title(\"Distribution of Car Prices\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U6gWPXDZXCEK",
   "metadata": {
    "id": "U6gWPXDZXCEK"
   },
   "source": [
    "separating numerical column and categorical column:\n",
    "\n",
    "this part shows the numerical columns and categorical features columns we have. Doing this made me understand the if there will be need for data encoding or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mxNAKIhCW_82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxNAKIhCW_82",
    "outputId": "24bb805b-0b9c-4e3d-c269-051ea22aa2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: Index(['id', 'model_year', 'milage', 'price'], dtype='object')\n",
      "Categorical columns: Index(['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col',\n",
      "       'int_col', 'accident', 'clean_title'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  for train data\n",
    "\n",
    "numerical_columns = df_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_columns = df_test.select_dtypes(include=[\"object\", \"bool\", \"category\"]).columns\n",
    "\n",
    "print(\"Numerical columns:\", numerical_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this plot will show the 'brand' category showing that a small number of dataset dominate compare to other brands\n",
    "\n",
    "# this was conducted to identify dominant and rare brands, guiding the choice of encoding strategies and dimensionality control. we can also change the column to other categorical columns too for better encoding strategies decision.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_train[\"brand\"].value_counts().head(10).plot(kind=\"bar\")\n",
    "plt.title(\"Top 10 Car Brands\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between price and fuel type was analyzed to verify its influence on vehicle value which justified categorical encoding.\n",
    "# Encoding is important so that the model used will be able to understand the differences between the fuel_types as it one of the features that the target depends on.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_train.boxplot(column=\"price\", by=\"fuel_type\")\n",
    "plt.title(\"Price vs Fuel Type\")\n",
    "plt.suptitle(\"\")  # removes automatic subtitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXcH7MzsiR5z",
   "metadata": {
    "id": "cXcH7MzsiR5z"
   },
   "source": [
    "Missing values\n",
    "\n",
    "the pandas library provides isnull() to check the column with missing values, sum() will give the total sum of the missing values if any exist,  and the sort_values() for sorting either in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1QV6bKFXidYL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "1QV6bKFXidYL",
    "outputId": "081f9fee-4f40-44d1-bd5e-c8a05d78b599"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuel_type', 'accident', 'clean_title']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pandas dataframe, we can use the .mean() method to find the mean for the each colums with missing values, then convert it to percentage multiplying be 100.\n",
    "\n",
    "#  Missing value analysis was performed to quantify data incompleteness and justify the imputation strategies applied during preprocessing.\n",
    "\n",
    "#  for train data\n",
    "\n",
    "train_missing_values =df_train.isnull().sum().sort_values(ascending=False)\n",
    "train_missing_columns = df_train.columns[df_train.isna().any()].tolist()\n",
    "\n",
    "train_missing_percentage = df_train.isnull().mean().sort_values(ascending=False) * 100\n",
    "\n",
    "train_missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "htMglwcbiqbp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "collapsed": true,
    "id": "htMglwcbiqbp",
    "outputId": "a5836513-ced2-46c8-e146-a678e94c9564"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_title    11.360876\n",
       "fuel_type       2.696080\n",
       "accident        1.300568\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for test data\n",
    "\n",
    "df_test.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "train_missing_percentage[train_missing_percentage > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the plot for columns with missing data\n",
    "(df_train.drop(['price'], axis=1).isnull().mean() * 100).plot(kind=\"bar\")\n",
    "plt.title(\"Missing values\")\n",
    "plt.ylabel(\"missing\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f837fd9",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc944a",
   "metadata": {},
   "source": [
    "## Filling missing data\n",
    "\n",
    "From using info() method when doing EDA above on the train data, It was seen that there are some missing values. Like in the fuel_type, accident and the clean_title features.\n",
    "\n",
    "This missing values are categorical features, therefore there is need to fill the the missing values before training because model can not be trained without with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c60432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For replacing the missing values in the training set, i used the mode() function from pandas to get the mode and using fillna() function from pandas library  to fill the positions where there was a misiing values.\n",
    "\n",
    "for column in train_missing_columns:\n",
    "    #  calculates the mode of the values.\n",
    "    mode_value = df_train[column].mode()[0]\n",
    "\n",
    "    # then filled it to the missing values position.\n",
    "    df_train[column] = df_train[column].fillna(mode_value)\n",
    "\n",
    "# For the test set, i filled only missing values for columns that are present in the train data set and excluding 'price'.\n",
    "# Doing this bring cosistency and more accuracy when training. As we are dealing with unseen data, we can't be sure if the unseen data will have additional column or not, or if they will contain some missing data, so it is best to take care of that. \n",
    " \n",
    "#  this line will select only the column with missing data in test data that are also present in the train data \n",
    "missing_columns_test = [col for col in train_missing_columns if col in df_test.columns]\n",
    "\n",
    "for column in missing_columns_test:\n",
    "    # Use the mode calculated from the training set for consistency\n",
    "    mode_value = df_train[column].mode()[0]\n",
    "    df_test[column] = df_test[column].fillna(mode_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87116650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 188533 entries, 0 to 188532\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   id            188533 non-null  int64 \n",
      " 1   brand         188533 non-null  object\n",
      " 2   model         188533 non-null  object\n",
      " 3   model_year    188533 non-null  int64 \n",
      " 4   milage        188533 non-null  int64 \n",
      " 5   fuel_type     188533 non-null  object\n",
      " 6   engine        188533 non-null  object\n",
      " 7   transmission  188533 non-null  object\n",
      " 8   ext_col       188533 non-null  object\n",
      " 9   int_col       188533 non-null  object\n",
      " 10  accident      188533 non-null  object\n",
      " 11  clean_title   188533 non-null  object\n",
      " 12  price         188533 non-null  int64 \n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 18.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#confirming if the missing data was successfully filled\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27103b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230c025",
   "metadata": {},
   "source": [
    "## Encoding Categorical Features\n",
    "\n",
    "it was seen from previous that there are some categorical features present in our datasets, there these categorical need to be encoded to be numerical features.\n",
    "\n",
    "To do this, i used the get_dummies() method that implements OneHotEncoding of categorical data from the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb505fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_encoded = pd.get_dummies(df_train, drop_first=True)\n",
    "\n",
    "df_test_encoded = pd.get_dummies(df_test, drop_first=True)\n",
    "df_test_encoded = df_test_encoded.reindex(columns=df_train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2bcbe",
   "metadata": {},
   "source": [
    "spliting the trian data into features (X) and target (y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aa302cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_encoded.drop(['price'], axis=1)\n",
    "y = df_train_encoded['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25696e79",
   "metadata": {},
   "source": [
    "### Cell 52\n",
    "Scales features, log-transforms the target, builds polynomial features, and prepares test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5970a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "standard_Scaler = StandardScaler().fit(X)\n",
    "\n",
    "df_train_scaled = pd.DataFrame(standard_Scaler.transform(X), columns = X.columns)\n",
    "df_test_scaled = standard_Scaler.fit_transform(df_test_encoded)\n",
    "df_train_scaled.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "# set and fit the scaler\n",
    "delta = 1\n",
    "y_logged =  np.log(y+delta)\n",
    "\n",
    "polynimial_features = PolynomialFeatures(degree=2, include_bias=False).fit(df_train_scaled)\n",
    "\n",
    "df_train_scaled_polynomial = pd.DataFrame(polynimial_features.transform(df_train_scaled), columns = polynimial_features.get_feature_names_out(df_train_scaled.columns))\n",
    "\n",
    "df_train_scaled_polynomial.plot(kind='box', figsize=(20,5))\n",
    "\n",
    "df_train_scaled_polynomial['price'] = y\n",
    "\n",
    "# spliting\n",
    "X_scaled = df_train_scaled_polynomial.drop(['price'], axis=1)\n",
    "y_scaled = df_train_scaled_polynomial['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_logged, train_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eec337",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "After I analyzed the Data, filled the columns with missing values and encoded the categorical features to numerical features. Then i moved to training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784ff00",
   "metadata": {},
   "source": [
    "Baseline models without tuning or applying polynomial features or log transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12039e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "base_models = {\n",
    "    \"Linear regression\": LinearRegression(),\n",
    "    \"Decision tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"k-NN\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "base_models_information = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cda130",
   "metadata": {},
   "source": [
    "Defines a helper to train a model, compute RMSE/R^2, and plot predicted vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval_plot(name, model, ax):\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "\n",
    "    base_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, base_pred))\n",
    "\n",
    "    ax.scatter(y_test, base_pred, c='g', alpha=0.5, label='Predicted')\n",
    "    min_val = min(y_test.min(), base_pred.min())\n",
    "    max_val = max(y_test.max(), base_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], c='r', label='Ideal')\n",
    "\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_title(f'{name} / RMSE={rmse} / score={score}')\n",
    "    ax.legend()\n",
    "\n",
    "    return {\"model\": name, \"rmse\": rmse, \"pred\": base_pred}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef569546",
   "metadata": {},
   "source": [
    "Runs the baseline Linear Regression model and plots predicted vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e006fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Linear regression\", base_models[\"Linear regression\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24986657",
   "metadata": {},
   "source": [
    "Runs the baseline Decision Tree model and plots predicted vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea48828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Decision tree\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Decision tree\", base_models[\"Decision tree\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84129a26",
   "metadata": {},
   "source": [
    "Runs the baseline Random Forest model and plots predicted vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adaf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Random forest\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"Random Forest\", base_models[\"Random Forest\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51285ee5",
   "metadata": {},
   "source": [
    "Runs the baseline k-NN model and plots predicted vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  K-Nearest Neighbors\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "base_models_information.append(fit_eval_plot(\"k-NN\", base_models[\"k-NN\"], ax))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54af662",
   "metadata": {},
   "source": [
    "the plots from base models above are underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da0a0e",
   "metadata": {},
   "source": [
    "Features scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3a94f",
   "metadata": {},
   "source": [
    "Scales features, log-transforms the target, builds polynomial features, and prepares test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee371ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# scale using the training split to avoid leakage\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), columns=X_test.columns, index=X_test.index\n",
    ")\n",
    "\n",
    "df_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(df_test_encoded), columns=df_test_encoded.columns, index=df_test_encoded.index\n",
    ")\n",
    "\n",
    "X_train_scaled.plot(kind=\"box\", figsize=(20, 5))\n",
    "\n",
    "# log-transform target for modeling\n",
    "y_train = np.log1p(y_train)\n",
    "y_test = np.log1p(y_test)\n",
    "\n",
    "# polynomial features based on the scaled training data\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train = pd.DataFrame(\n",
    "    poly.fit_transform(X_train_scaled),\n",
    "    columns=poly.get_feature_names_out(X_train_scaled.columns),\n",
    "    index=X_train_scaled.index,\n",
    ")\n",
    "X_test = pd.DataFrame(\n",
    "    poly.transform(X_test_scaled),\n",
    "    columns=poly.get_feature_names_out(X_train_scaled.columns),\n",
    "    index=X_test_scaled.index,\n",
    ")\n",
    "\n",
    "X_train.plot(kind=\"box\", figsize=(20, 5))\n",
    "\n",
    "df_test_poly = pd.DataFrame(\n",
    "    poly.transform(df_test_scaled),\n",
    "    columns=poly.get_feature_names_out(df_test_scaled.columns),\n",
    "    index=df_test_scaled.index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e151aea8",
   "metadata": {},
   "source": [
    "Models Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481567b",
   "metadata": {},
   "source": [
    "Defines grid search helper with CV RMSE and predicted vs actual plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677963d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "results = []\n",
    "best_models = {}\n",
    "scoring = \"neg_root_mean_squared_error\"\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def run_gridsearch(name, model, grid, scoring):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        # n_jobs=-1,\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    rmse = -gs.best_score_\n",
    "    results.append({\"model\": name, \"rmse\": rmse, \"params\": gs.best_params_})\n",
    "    best_models[name] = gs.best_estimator_\n",
    "    params = gs.best_params_\n",
    "    print(f\"{name} | RMSE={rmse:.4f}\")\n",
    "    print(f\"params={params} \")\n",
    "\n",
    "    # Predicted vs actual plot for the best model\n",
    "    preds = gs.best_estimator_.predict(X_test)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    ax.scatter(y_test, preds, c='g', alpha=0.5, label='Predicted')\n",
    "    min_val = min(y_test.min(), preds.min())\n",
    "    max_val = max(y_test.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], c='r', label='Ideal')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_title(f\"{name} (best) / CV_RMSE={rmse:.4f}\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b94998",
   "metadata": {},
   "source": [
    "Runs grid search for Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_grid = {\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"positive\": [False, True]\n",
    "}\n",
    "\n",
    "run_gridsearch(\"linreg\", LinearRegression(), linreg_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366cdf0",
   "metadata": {},
   "source": [
    "Runs grid search for k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = {\n",
    "    \"n_neighbors\": [5, 10],\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"metric\": [\"minkowski\"],\n",
    "    \"p\": [2],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"knn\", KNeighborsRegressor(), knn_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8524a",
   "metadata": {},
   "source": [
    "Runs grid search for Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid = {\n",
    "    \"max_depth\": [None, 10],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"ccp_alpha\": [0.0],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"dt\", DecisionTreeRegressor(random_state=42), dt_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489ea62",
   "metadata": {},
   "source": [
    "Runs grid search for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fe470",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [100],\n",
    "    \"max_depth\": [None, 20],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"bootstrap\": [True],\n",
    "}\n",
    "\n",
    "run_gridsearch(\"rf\", RandomForestRegressor(random_state=42), rf_grid, scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed5420",
   "metadata": {},
   "source": [
    "Builds and displays the sorted grid-search results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afca2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(\"rmse\")\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c45e87",
   "metadata": {},
   "source": [
    "Kaggle Submission File Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2a896",
   "metadata": {},
   "source": [
    "Generates submission predictions from the best model and saves submission.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406eb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the best model\n",
    "best_model_name = results_df.iloc[0][\"model\"]\n",
    "\n",
    "# Retrieve the best estimator for that model\n",
    "best = best_models[best_model_name]\n",
    "\n",
    "# Keep test ids from the raw test set\n",
    "test_ids = df_test[\"id\"].copy()\n",
    "\n",
    "# Use the same feature set used for training\n",
    "try:\n",
    "    X_submit = df_test_poly\n",
    "except NameError:\n",
    "    X_submit = df_test_encoded\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best.predict(X_submit)\n",
    "\n",
    "# If target was log-transformed, invert back to price scale\n",
    "use_log_target = y_train.max() < 1000\n",
    "if use_log_target:\n",
    "    predictions = np.expm1(predictions)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({\"id\": test_ids, \"price\": predictions})\n",
    "\n",
    "# Ensure prices are non-negative, as car prices cannot be negative\n",
    "submission_df[\"price\"] = submission_df[\"price\"].clip(lower=0)\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "display(submission_df.head())\n",
    "\n",
    "# Save to CSV for Kaggle submission\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
